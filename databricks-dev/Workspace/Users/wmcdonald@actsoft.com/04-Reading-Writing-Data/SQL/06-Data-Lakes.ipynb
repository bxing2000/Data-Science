{"cells":[{"cell_type":"markdown","source":["# Querying Data Lakes with DataFrames\n\nApache Spark&trade; and Azure Databricks&reg; make it easy to access and work with files stored in Data Lakes, such as Azure Data Lake Storage (ADLS).\n\nCompanies frequently store thousands of large data files gathered from various teams and departments, typically using a diverse variety of formats including CSV, JSON, and XML. Data scientists often wish to extract insights from this data.\n\nThe classic approach to querying this data is to load it into a central database called a **data warehouse**. Traditionally, data engineers must design the schema for the central database, extract the data from the various data sources, transform the data to fit the warehouse schema, and load it into the central database. A data scientist can then query the data warehouse directly or query smaller data sets created to optimize specific types of queries. The data warehouse approach works well, but requires a great deal of up front effort to design and populate schemas. It also limits historical data, which is constrained to only the data that fits the warehouseâ€™s schema.\n\nAn alternative approach is a **Data Lake**, which:\n\n* Is a storage repository that cheaply stores a vast amount of raw data in its native format.\n* Consists of current and historical data dumps in various formats including XML, JSON, CSV, Parquet, etc.\n* May contain operational relational databases with live transactional data.\n\nSpark is ideal for querying Data Lakes. Spark DataFrames can be used to read directly from raw files contained in a Data Lake and then execute queries to join and aggregate the data.\n\nThis lesson illustrates how to perform exploratory data analysis (EDA) to gain insights from a Data Lake.\n\n## Prerequisites\n* **IMPORTANT**: You must have permissions within your Azure subscription to create an App Registration and service principal within Azure Active Directory to complete this lesson.\n* Lesson: <a href=\"$./02-Querying-Files\">Querying Files with SQL</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Create Azure Data Lake Storage Gen1 (ADLS)\n\n1. In the [Azure portal](https://portal.azure.com), select **+ Create a resource**, enter \"data lake\" into the Search the Marketplace box, select **Data Lake Storage Gen1** from the results, and then select **Create**.\n\n   ![In the Azure portal, +Create a resource is highlighted in the navigation pane, \"data lake\" is entered into the Search the Marketplace box, and Data Lake Storage Gen1 is highlighted in the results.](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-create-resource.png 'Create Azure Data Lake Storage Gen1')\n\n2. On the New Data Lake Storage Gen1 blade, enter the following:\n\n   - **Name**: Enter a globally unique name (indicated by a green check mark).\n   - **Subscription**: Select the subscription you are using for this module.\n   - **Resource group**: Choose your module resource group.\n   - **Location**: Select the closest location.\n   - **Pricing package**: Choose Pay-as-You-Go.\n   - **Encryption settings**: Leave set to the default value of Enabled.\n\n   ![The New Data Lake Storage Gen1 blade is displayed, with the previously mentioned settings entered into the appropriate fields.](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-create-new.png 'New Data Lake Storage Gen1')\n\n3. Select **Create** to provision the new ADLS instance.\n\n4. In the cell below, set the value of the `adlsAccountName` variable to the same name you used for the **Name** field when creating your ADLS instance above, and then run the cell."],"metadata":{}},{"cell_type":"code","source":["adlsAccountName = \"warrentestgen1\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Create Azure Active Directory application and service principal\n\n> **IMPORTANT**: You must have permissions within your Azure subscription to create an App registration and service principal within Azure Active Directory to complete this lesson.\n\nADLS uses Azure Active Directory for authentication. To provide access to your ADLS instance from Azure Databricks, you will use [service-to-service authentication](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-using-active-directory). For this, you need to create an identity in Azure Active Directory (Azure AD) known as a service principal.\n\n1. In the [Azure portal](https://portal.azure.com), select **Azure Active Directory** from the left-hand navigation menu, select **App registrations**, and then select **+ New application registration**.\n\n   ![Register new app in Azure Active Directory](https://databricksdemostore.blob.core.windows.net/images/04/06/aad-app-registration.png 'Register new app in Azure Active Directory')\n\n2. On the Create blade, enter the following:\n\n  * **Name**: Enter a unique name, such as databricks-demo (this name must be unique, as indicated by a green check mark).\n  * **Application type**: Select Web app / API.\n  * **Sign-on URL**: Enter https://databricks-demo.com.\n\n   ![Create a new app registration](https://databricksdemostore.blob.core.windows.net/images/04/06/aad-app-create.png 'Create a new app registration')\n\n3. Select **Create**.\n\n4. To access your ADLS instance from Azure Databricks you will need to provide the credentials of your newly created service principal within Databricks. On the Registered app blade that appears, copy the **Application ID** and paste it into the cell below as the value for the `clientId` variable.\n\n   ![Copy the Registered App Application ID](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-id.png 'Copy the Registered App Application ID')\n\n5. Next, select **Settings** on the Registered app blade, and then select **Keys**.\n\n   ![Open Keys blade for the Registered App](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-settings-keys.png 'Open Keys blade for the Registered App')\n\n6. On the Keys blade, you will create a new password by doing the following under Passwords:\n\n  * **Description**: Enter a description, such as ADLS Auth.\n  * **Expires**: Select a duration, such as In 1 year.\n\n  ![Create new password](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-create-key.png 'Create new password')\n\n7. Select **Save**, and then copy the key displayed under **Value**, and paste it into the cell below for the value of the `clientKey` variable. **Note**: This value will not be accessible once you navigate away from this screen, so make sure you copy it before leaving the Keys blade.\n\n  ![Copy key value](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-key-value.png 'Copy key value')\n\n8. Run the cell below."],"metadata":{}},{"cell_type":"code","source":["clientId = \"9a14adb0-d7a6-4021-80f3-f47115fc1597\"\nclientKey = \"F21cE5gcgIlP.y8ZCH-hSmt]hEGug[fn\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Retrieve your Azure AD tenant ID\n\nTo perform authentication using the service principal account, Databricks uses OAUTH2. For this, you need to provide your Azure AD Tenant ID.\n\n1. To retrieve your tenant ID, select **Azure Active Directory** from the left-hand navigation menu in the Azure portal, then select **Properties**, and select the copy button next to **Directory ID** on the Directory Properties blade.\n\n   ![Retrieve Tenant ID](https://databricksdemostore.blob.core.windows.net/images/04/06/aad-tenant-id.png 'Retrieve Tenant ID')\n\n2. Paste the copied value into the cell below for the value of the `tenantId` variable, and then run the cell."],"metadata":{}},{"cell_type":"code","source":["tenantId = \"1a5d5bac-c4df-4a2c-9374-267fcff8eead\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## Assign permissions to the service principal in ADLS\n\nNext, you need to assign the required permissions to the service principal in ADLS.\n\n1. In the [Azure portal](https://portal.azure.com), navigate to the ADLS instance you created above, and on the Overview blade, select **Data explorer**.\n\n   ![ADLS Overview blade](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-overview.png 'ADLS Overview blade')\n\n2. In the Data Explorer blade, select **Access** on the toolbar.\n\n   ![ADLS Data Explorer toolbar](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-data-explorer-toolbar.png 'ADLS Data Explorer toolbar')\n\n3. On the Access blade, select **+ Add**.\n\n   ![ADLS Data Explorer add access](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-access.png 'ADLS Data Explorer add access')\n\n4. On the Assign permissions -> Select user or group blade, enter the name of your Registered app (e.g., databricks-demo) into the **Select** box, choose your app from the list, and select **Select**.\n\n   ![ADLS assign permissions to user or group](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-assign-permissions-select-user-or-group.png 'ADLS assign permissions to user or group')\n\n5. On the Assign permissions -> Select permissions blade, set the following:\n\n  * **Permissions**: Check **Read**, **Write**, and **Execute**.\n  * **Add to**: Choose This folder and all children.\n  * **Add as**: Choose An access permission entry.\n  \n  ![ADLS assign permissions](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-assign-permissions.png 'ADLS assign permissions')\n\n6. Select **Ok**\n\n7. You will now see the service principal listed under **Assigned permissions** on the Access blade.\n\n  ![ADLS assigned permissions](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-assigned-permissions.png 'ADLS assigned permissions')"],"metadata":{}},{"cell_type":"markdown","source":["## Mount ADLS to DBFS\n\nYou are now ready to access your ADLS account from Azure Databricks. Run the cell below to set the required configuration and mount ADLS to DBFS."],"metadata":{}},{"cell_type":"code","source":["configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n           \"dfs.adls.oauth2.client.id\": clientId,\n           \"dfs.adls.oauth2.credential\": clientKey,\n           \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/\" + tenantId + \"/oauth2/token\"}\n\ndbutils.fs.mount(\n  source = \"adl://warrentestgen1.azuredatalakestore.net/\",\n  mount_point = \"/mnt/adls\",\n  extra_configs = configs)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4180736491747904&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>   source <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;adl://warrentestgen1.azuredatalakestore.net/&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>   mount_point <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/adls&#34;</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\">   extra_configs = configs)\n</span>\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1588856738755-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 314</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    315</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o323.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/adls; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/adls\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:465)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/adls\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:224)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:326)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:220)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:79)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:304)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:282)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:51)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:78)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:78)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:47)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:428)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:14)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:14)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:409)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:336)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:14)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:46)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:611)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:611)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:534)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:321)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:152)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:152)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:310)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:217)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["## Copy data to ADLS\n\nRun the following cell to copy the Crime-data-2016 dataset from the Training folder into your ADLS instance, in a folder named \"training\". This will take a few minutes to complete."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.cp(\"/mnt/training/crime-data-2016\", \"mnt/adls/training/crime-data-2016\", True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4180736491747906&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>cp<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/mnt/training/crime-data-2016&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;mnt/adls/training/crime-data-2016&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1588708292664-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 314</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    315</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.cp.\n: com.microsoft.azure.datalake.store.ADLException: Error creating directory /training/crime-data-2016\nOperation MKDIRS failed with exception java.net.UnknownHostException : warretest.azuredatalakestore.net\nLast encountered exception thrown after 5 tries. [java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException]\n [ServerRequestId:null]\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.createDirectory(ADLStoreClient.java:589)\n\tat com.databricks.adl.AdlFileSystem.mkdirs(AdlFileSystem.java:581)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7$$anonfun$apply$mcZ$sp$8.apply$mcZ$sp(DatabricksFileSystemV2.scala:743)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7$$anonfun$apply$mcZ$sp$8.apply(DatabricksFileSystemV2.scala:741)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7$$anonfun$apply$mcZ$sp$8.apply(DatabricksFileSystemV2.scala:741)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7.apply$mcZ$sp(DatabricksFileSystemV2.scala:741)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7.apply(DatabricksFileSystemV2.scala:741)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1$$anonfun$apply$mcZ$sp$7.apply(DatabricksFileSystemV2.scala:741)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$withUserContextRecorded$1.apply(DatabricksFileSystemV2.scala:936)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withUserContextRecorded(DatabricksFileSystemV2.scala:909)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1.apply$mcZ$sp(DatabricksFileSystemV2.scala:740)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1.apply(DatabricksFileSystemV2.scala:740)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$mkdirs$1.apply(DatabricksFileSystemV2.scala:740)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:440)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:421)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.mkdirs(DatabricksFileSystemV2.scala:739)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.mkdirs(DatabricksFileSystem.scala:198)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$cpRecursive(DBUtilsCore.scala:177)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1$$anonfun$apply$3.apply(DBUtilsCore.scala:118)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1$$anonfun$apply$3.apply(DBUtilsCore.scala:113)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1.apply(DBUtilsCore.scala:113)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$cp$1.apply(DBUtilsCore.scala:113)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.cp(DBUtilsCore.scala:112)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.cp(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.UnknownHostException: warretest.azuredatalakestore.net\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n\tat sun.net.www.protocol.https.HttpsClient.&lt;init&gt;(HttpsClient.java:264)\n\tat sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367)\n\tat sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1162)\n\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1046)\n\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1044)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1043)\n\tat sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177)\n\tat sun.net.www.protocol.http.HttpURLConnection.getOutputStream0(HttpURLConnection.java:1340)\n\tat sun.net.www.protocol.http.HttpURLConnection.access$100(HttpURLConnection.java:92)\n\tat sun.net.www.protocol.http.HttpURLConnection$8.run(HttpURLConnection.java:1307)\n\tat sun.net.www.protocol.http.HttpURLConnection$8.run(HttpURLConnection.java:1305)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.getOutputStream(HttpURLConnection.java:1304)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getOutputStream(HttpsURLConnectionImpl.java:264)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:273)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91)\n\tat com.microsoft.azure.datalake.store.Core.mkdirs(Core.java:399)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.createDirectory(ADLStoreClient.java:587)\n\t... 51 more\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Looking at the files in our Data Lake\n\nStart by reviewing which files are in our Data Lake.\n\nIn `dbfs:/mnt/adls/training/crime-data-2016`, there are Parquet files containing 2016 crime data from several United States cities.\n\nIn the cell below we have data for Boston, Chicago, New Orleans, and more."],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/adls/training/crime-data-2016"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Operation LISTSTATUS failed with exception java.net.UnknownHostException : warretest.azuredatalakestore.net\nLast encountered exception thrown after 5 tries. [java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException]\n [ServerRequestId:null]\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectoryInternal(ADLStoreClient.java:558)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:534)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:398)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:384)\n\tat com.databricks.adl.AdlFileSystem.listStatus(AdlFileSystem.java:463)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1$$anonfun$apply$2.apply(DatabricksFileSystemV2.scala:95)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1$$anonfun$apply$2.apply(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1.apply(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1.apply(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:440)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:421)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:91)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:150)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$ls$1.apply(DBUtilsCore.scala:86)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$ls$1.apply(DBUtilsCore.scala:85)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:1)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:44)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:46)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:48)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw.&lt;init&gt;(command-4180736491747908:50)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw.&lt;init&gt;(command-4180736491747908:52)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read.&lt;init&gt;(command-4180736491747908:54)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$.&lt;init&gt;(command-4180736491747908:58)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$.&lt;clinit&gt;(command-4180736491747908)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$eval$.$print(&lt;notebook&gt;:6)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:699)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:652)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.UnknownHostException: warretest.azuredatalakestore.net\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n\tat sun.net.www.protocol.https.HttpsClient.&lt;init&gt;(HttpsClient.java:264)\n\tat sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367)\n\tat sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1162)\n\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1046)\n\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1044)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1043)\n\tat sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1570)\n\tat sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:92)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1490)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1488)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1487)\n\tat java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:352)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:288)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91)\n\tat com.microsoft.azure.datalake.store.Core.listStatus(Core.java:803)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectoryInternal(ADLStoreClient.java:556)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:534)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:398)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.enumerateDirectory(ADLStoreClient.java:384)\n\tat com.databricks.adl.AdlFileSystem.listStatus(AdlFileSystem.java:463)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1$$anonfun$apply$2.apply(DatabricksFileSystemV2.scala:95)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1$$anonfun$apply$2.apply(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1.apply(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.backend.daemon.data.client.DBFSV2$$anonfun$listStatus$1.apply(DatabricksFileSystemV2.scala:92)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:440)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:421)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:91)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:150)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$ls$1.apply(DBUtilsCore.scala:86)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$$anonfun$ls$1.apply(DBUtilsCore.scala:85)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.com$databricks$backend$daemon$dbutils$FSUtils$$withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.ls(DBUtilsCore.scala:85)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:34)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:1)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:44)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:46)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw$$iw.&lt;init&gt;(command-4180736491747908:48)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw$$iw.&lt;init&gt;(command-4180736491747908:50)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$$iw.&lt;init&gt;(command-4180736491747908:52)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read.&lt;init&gt;(command-4180736491747908:54)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$.&lt;init&gt;(command-4180736491747908:58)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$read$.&lt;clinit&gt;(command-4180736491747908)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$eval$.$print(&lt;notebook&gt;:6)\n\tat line1420c957e3cc45cd871ddb783dd97aa233.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:699)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:652)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:202)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["The next step in looking at the data is to create a temporary view for each file.  Recall that temporary views use a similar syntax to `CREATE TABLE` but using the command `CREATE TEMPORARY VIEW`.  Temporary views are removed once your session has ended while tables are persisted beyond a given session.\n\nStart by creating a view of the data from New York and then Boston:\n\n| City          | Table Name              | Path to DBFS file\n| ------------- | ----------------------- | -----------------\n| **New York**  | `CrimeDataNewYork`      | `dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-New-York-2016.parquet`\n| **Boston**    | `CrimeDataBoston`       | `dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-Boston-2016.parquet`"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW CrimeDataNewYork\n  USING parquet\n  OPTIONS (\n    path \"dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-New-York-2016.parquet\"\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\ncom.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: com.microsoft.azure.datalake.store.ADLException: Error getting info for file /training/crime-data-2016/Crime-Data-New-York-2016.parquet\nOperation GETFILESTATUS failed with exception java.net.UnknownHostException : warretest.azuredatalakestore.net\nLast encountered exception thrown after 5 tries. [java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException,java.net.UnknownHostException]\n [ServerRequestId:null]\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:737)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718)\n\tat com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:446)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:759)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:756)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:119)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:756)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:756)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:440)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:421)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:450)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:755)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:201)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:613)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:597)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:597)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:391)\n\tat org.apache.spark.sql.execution.datasources.CreateTempViewUsing.run(ddl.scala:101)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:206)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:206)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:171)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:206)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:88)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal$$anonfun$1.apply(SQLDriverLocal.scala:34)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:34)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.UnknownHostException: warretest.azuredatalakestore.net\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:607)\n\tat sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:666)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:463)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:558)\n\tat sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:264)\n\tat sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:367)\n\tat sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:191)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1162)\n\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1046)\n\tat sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1044)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1043)\n\tat sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:177)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1570)\n\tat sun.net.www.protocol.http.HttpURLConnection.access$200(HttpURLConnection.java:92)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1490)\n\tat sun.net.www.protocol.http.HttpURLConnection$9.run(HttpURLConnection.java:1488)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.AccessController.doPrivilegedWithCombiner(AccessController.java:784)\n\tat sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1487)\n\tat java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)\n\tat sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:352)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:288)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91)\n\tat com.microsoft.azure.datalake.store.Core.getFileStatus(Core.java:655)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:735)\n\t... 73 more\n\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:126)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:141)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:385)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:362)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:251)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:246)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:288)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:362)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":18},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW CrimeDataBoston\n  USING parquet\n  OPTIONS (\n    path \"dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-Boston-2016.parquet\"\n  )"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["With the view created, it is now possible to review the first couple records of each file.\n\nNotice in the example below:\n* The `CrimeDataNewYork` and `CrimeDataBoston` datasets use different names for the columns\n* The data itself is formatted differently and different names are used for similar concepts\n\nThis is common in a Data Lake.  Often files are added to a Data Lake by different groups at different times.  While each file itself usually has clean data, there is little consistency across files.  The advantage of this strategy is that anyone can contribute information to the Data Lake and that Data Lakes scale to store arbitrarily large and diverse data.  The tradeoff for this ease in storing data is that it doesnâ€™t have the rigid structure of a more traditional relational data model so the person querying the Data Lake will need to clean the data before extracting useful insights.\n\nThe alternative to a Data Lake is a Data Warehouse.  In a Data Warehouse, a committee often regulates the schema and ensures data is cleaned before being made available.  This makes querying much easier but also makes gathering the data much more expensive and time-consuming.  Many companies choose to start with a Data Lake to accumulate data.  Then, as the need arises, they clean the data and produce higher quality tables for querying.  This reduces the upfront costs while still making data easier to query over time.  These cleaned tables can even be later loaded into a formal data warehouse through nightly batch jobs.  In this way, Apache Spark can be used to manage and query both Data Lakes and Data Warehouses."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM CrimeDataNewYork"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%sql\n\nSELECT * FROM CrimeDataBoston"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Same type of data, different structure\n\nIn this section, we examine crime data to figure out how to extract homicide statistics.\n\nBecause our data sets are pooled together in a Data Lake, each city may use different field names and values to indicate homicides, dates, etc.\n\nFor example:\n* Some cities use the value \"HOMICIDE\", \"CRIMINAL HOMICIDE\" or even \"MURDER\"\n* In New York, the column is named `offenseDescription` but, in Boston, the column is named `OFFENSE_CODE_GROUP`\n* In New York, the date of the event is in the `reportDate` column but, in Boston, there is a single column named `MONTH`"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\nTo get started, create a temporary view containing only the homicide-related rows.\n\nAt the same time, normalize the data structure of each table so that all the columns (and their values) line up with each other.\n\nIn the case of New York and Boston, here are the unique characteristics of each data set:\n\n| | Offense-Column        | Offense-Value          | Reported-Column  | Reported-Data Type |\n|-|-----------------------|------------------------|-----------------------------------|\n| New York | `offenseDescription`  | starts with \"murder\" or \"homicide\" | `reportDate`     | `timestamp`    |\n| Boston | `OFFENSE_CODE_GROUP`  | \"Homicide\"             | `MONTH`          | `integer`      |\n\nFor the upcoming aggregation, you will need to alter the New York data set to include a `month` column which can be computed from the `reportDate` column using the `month()` function. Boston already has this column.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> One helpful tool for finding the offences we're looking for is using <a href=\"https://en.wikipedia.org/wiki/Regular_expression\" target=\"_blank\">regular expressions</a> supported by SQL\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We can also normalize the values with the `CASE`, `WHEN`, `THEN` & `ELSE` expressions but that is not required for the task at hand."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW HomicidesNewYork AS\n  SELECT month(reportDate) AS month, offenseDescription AS offense\n  FROM CrimeDataNewYork\n  WHERE lower(offenseDescription) LIKE 'murder%' OR lower(offenseDescription) LIKE 'homicide%'"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW HomicidesBoston AS\n  SELECT month, OFFENSE_CODE_GROUP AS offense\n  FROM CrimeDataBoston\n  WHERE lower(OFFENSE_CODE_GROUP) = 'homicide'"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["You can see below that the structure of our two tables is now identical."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM HomicidesNewYork LIMIT 5"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql\n\nSELECT * FROM HomicidesBoston LIMIT 5"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## Analyzing the data"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nNow that we have normalized the homicide data for each city we can combine the two by taking their union.\n\nWhen we are done, we can then aggregate that data to compute the number of homicides per month.\n\nStart by creating a new view called `HomicidesBostonAndNewYork` which simply unions the result of two `SELECT` statements together.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://stackoverflow.com/questions/49925/what-is-the-difference-between-union-and-union-all\">this Stack Overflow post</a> for the difference between `UNION` and `UNION ALL`"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMPORARY VIEW HomicidesBostonAndNewYork AS\n  SELECT * FROM HomicidesNewYork\n    UNION ALL\n  SELECT * FROM HomicidesBoston"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["You can now see below all the data in one table:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT *\nFROM HomicidesBostonAndNewYork\nORDER BY month"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["And finally we can perform a simple aggregation to see the number of homicides per month:"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT month, count(*) AS homicides\nFROM HomicidesBostonAndNewYork\nGROUP BY month\nORDER BY month"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## Exercise 1\n\nMerge the crime data for Chicago with the data for New York and Boston and then update our final aggregation of counts-by-month."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nCreate the initial view of the Chicago data.\n0. The source file is `dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-Chicago-2016.parquet`\n0. Name the view `CrimeDataChicago`\n0. View the data with a simple `SELECT` statement"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\n<<FILL_IN>>"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\ntotal = spark.sql(\"select count(*) from CrimeDataChicago\").first()[0]\ndbTest(\"SQL-L6-crimeDataChicago-count\", 267872, total)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["-sandbox\n### Step 2\n\nCreate a new view that normalizes the data structure.\n0. Name the view `HomicidesChicago`\n0. The table should have at least two columns: `month` and `offense`\n0. Filter the data to only include homicides\n0. View the data with a simple `SELECT` statement\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You will need to use the `month()` function to extract the month-of-the-year.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** To find out which values for each offense constitutes a homicide, produce a distinct list of values from the table `CrimeDataChicago`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\n<<FILL_IN>>"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\nhomicidesChicago = spark.sql(\"SELECT month, count(*) FROM HomicidesChicago GROUP BY month ORDER BY month\").collect()\ndbTest(\"SQL-L6-homicideChicago-len\", 12, len(homicidesChicago))\n\ndbTest(\"SQL-L6-homicideChicago-0\", 54, homicidesChicago[0][1])\ndbTest(\"SQL-L6-homicideChicago-6\", 71, homicidesChicago[6][1])\ndbTest(\"SQL-L6-homicideChicago-11\", 58, homicidesChicago[11][1])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["-sandbox\n### Step 3\n\nCreate a new view that merges all three data sets (New York, Boston, Chicago):\n0. Name the view `AllHomicides`\n0. Use the `UNION ALL` expression introduced earlier to merge all three tables\n  * `HomicidesNewYork`\n  * `HomicidesBoston`\n  * `HomicidesChicago`\n0. View the data with a simple `SELECT` statement\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** To union three tables together, copy the previous example and just add as second `UNION` statement followed by the appropriate `SELECT` statement."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\n<<FILL_IN>>"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\nallHomicides = spark.sql(\"SELECT count(*) AS total FROM AllHomicides\").first().total\ndbTest(\"SQL-L6-allHomicides-count\", 1203, allHomicides)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### Step 4\n\nCreate a new view that counts the number of homicides per month.\n0. Name the view `HomicidesByMonth`\n0. Rename the column `count(1)` to `homicides`\n0. Group the data by `month`\n0. Sort the data by `month`\n0. Count the number of records for each aggregate\n0. View the data with a simple `SELECT` statement"],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\n\n<<FILL_IN>>"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["%python\n# TEST - Run this cell to test your solution.\n\nallHomicides = spark.sql(\"SELECT * FROM HomicidesByMonth\").collect()\ndbTest(\"SQL-L6-homicidesByMonth-len\", 12, len(allHomicides))\n\ndbTest(\"SQL-L6-homicidesByMonth-0\", 1, allHomicides[0].month)\ndbTest(\"SQL-L6-homicidesByMonth-11\", 12, allHomicides[11].month)\n\ndbTest(\"SQL-L6-allHomicides-0\", 83, allHomicides[0].homicides)\ndbTest(\"SQL-L6-allHomicides-1\", 68, allHomicides[1].homicides)\ndbTest(\"SQL-L6-allHomicides-2\", 72, allHomicides[2].homicides)\ndbTest(\"SQL-L6-allHomicides-3\", 76, allHomicides[3].homicides)\ndbTest(\"SQL-L6-allHomicides-4\", 105, allHomicides[4].homicides)\ndbTest(\"SQL-L6-allHomicides-5\", 120, allHomicides[5].homicides)\ndbTest(\"SQL-L6-allHomicides-6\", 116, allHomicides[6].homicides)\ndbTest(\"SQL-L6-allHomicides-7\", 144, allHomicides[7].homicides)\ndbTest(\"SQL-L6-allHomicides-8\", 109, allHomicides[8].homicides)\ndbTest(\"SQL-L6-allHomicides-9\", 109, allHomicides[9].homicides)\ndbTest(\"SQL-L6-allHomicides-10\", 111, allHomicides[10].homicides)\ndbTest(\"SQL-L6-allHomicides-11\", 90, allHomicides[11].homicides)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["## Unmount ADLS from DBFS"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.unmount(\"/mnt/adls\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["## Summary\n\n* Spark SQL allows you to easily manipulate data in a Data Lake\n* Temporary views help to save your cleaned data for downstream analysis"],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is a Data Lake?  \n**A:** Data Lakes are a loose collection of data files gathered from various sources.  Spark loads each file as a table and then executes queries joining and aggregating these files.\n\n**Q:** What are some advantages of Data Lakes over more classic Data Warehouses?  \n**A:** Data Lakes allow for large amounts of data to be aggregated from many sources with minimal ceremony or overhead.  Data Lakes also allow for very very large files.  Powerful query engines such as Spark can read the diverse collection of files and execute complex queries efficiently.\n\n**Q:** What are some advantages of Data Warehouses?  \n**A:** Data warehouses are neatly curated to ensure data from all sources fit a common schema.  This makes them very easy to query.\n\n**Q:** What's the best way to combine the advantages of Data Lakes and Data Warehouses?  \n**A:** Start with a Data Lake.  As you query, you will discover cases where the data needs to be cleaned, combined, and made more accessible.  Create periodic Spark jobs to read these raw sources and write new \"golden\" tables that are cleaned and more easily queried."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n* Continue to the [Azure Data Lake Storage Gen2]($./07-Azure-Data-Lake-Gen2) lesson"],"metadata":{}}],"metadata":{"name":"06-Data-Lakes","notebookId":4180736491747892},"nbformat":4,"nbformat_minor":0}