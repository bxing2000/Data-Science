{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"},"colab":{"name":"2.4 [HANDS ON] CreatingGoodFeatures.ipynb","provenance":[{"file_id":"1CPLnueTLbSWzvXmb6yaZ2Vwu8kiFD3yA","timestamp":1605642484473}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ixSakInOjKTr"},"source":["# Creating Good Features"]},{"cell_type":"code","metadata":{"id":"l_NIINPptWGq"},"source":["import warnings\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","from sklearn.preprocessing import scale, MinMaxScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score, f1_score\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer, label_binarize\n","\n","import matplotlib.pyplot as plt\n","\n","from google.cloud import bigquery as bq\n","\n","warnings.filterwarnings('ignore')\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfsQlaw4tbfS","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1579026123928,"user_tz":300,"elapsed":39296,"user":{"displayName":"Alessandro Bersia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDkCN4chtML2Qb9rSdZ1jkz8R55jRL3E0zW6cr5=s64","userId":"01884509692445689789"}},"outputId":"4ecff0a7-35f1-4850-d789-93ce3e014fef"},"source":["from google.colab import auth\n","auth.authenticate_user()\n","print('Authenticated')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Authenticated\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pJB9fGhdtWGx"},"source":["# Import the data"]},{"cell_type":"code","metadata":{"id":"aaIwwTIatWGz"},"source":["SQL = \"\"\"\n","SELECT DriverId\n","     , TripNumber\n","     , Distance\n","     , TripType\n","     , DayOfWeek\n","     , HourOfDay\n","     , AvgSpeed\n","     , Loaded\n","  FROM `geotab-bootcamp.DemoData.PredictingLoaded2`\n"," ORDER BY RAND()\n","\"\"\"\n","\n","df = bq.Client(project='geotab-bootcamp').query(SQL).to_dataframe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vj-t93CItWG7"},"source":["# First look at the DataFrame"]},{"cell_type":"code","metadata":{"id":"7jJK8oFStWG8"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_in5NKnptWHA"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6D2YDrNJtWHE"},"source":["We can see that there are 15,895 rows and 8 columns. We can already notice a few things:\n","1. We have both numerical and non-numerical columns\n","2. We have both categorical and continuous data\n","3. It looks like some of the columns have missing values\n","\n","Let's first take a look at the head of the DataFrame."]},{"cell_type":"code","metadata":{"id":"Ocrv5mbdtWHH"},"source":["df.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmO7gXCmtWHO"},"source":["We can see that TripType, and DayOfWeek are categorical and included as strings. Our algorithms can't take strings as input so we'll have to encode those somehow. We see that hour of day is numeric and we should double-check that it ranges from 0 to 23."]},{"cell_type":"code","metadata":{"id":"qcz0kcGstWHR"},"source":["print('Min: ', df['HourOfDay'].min(), ', Max: ', df['HourOfDay'].max(), end='', sep='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlaTSG2BtWHW"},"source":["The column *Loaded* is the one we will be trying to predict, based on the other columns. Are all of these columns useful? \n","\n","Do you think DriverId or TripNumber would be related to whether the vehicle is Loaded or Unloaded? Why or why not? How would you check?\n","\n","Before any modelling happens, we need to do more quality checks on our data. We already know there are missing values and we'll have to address that somehow. Let's first see how many missing values we have."]},{"cell_type":"code","metadata":{"id":"-TwTMd8gtWHX"},"source":["df[df.isna().any(axis=1)].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SyYS68-GtWHf"},"source":["So we have some rows that have a missing value in at least one of the columns. What about outliers? Let's describe our numeric columns and see what we find."]},{"cell_type":"code","metadata":{"id":"tLejRKe5tWHh"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E83t_NYptWHl"},"source":["We can see that most of our columns look good, but there's something odd about AvgSpeed. It's pretty unlikely that a driving trip would have an average speed of over 200 km/hr. Let's look at a histogram of AvgSpeed and take a closer look."]},{"cell_type":"code","metadata":{"id":"XQQ76_U_tWHm"},"source":["fig = plt.figure(figsize=(12, 4))\n","ax = fig.add_subplot(111)\n","_ = ax.hist(df['AvgSpeed'], bins=20, color='#00aeef', edgecolor='white', zorder=4)\n","_ = ax.grid(linestyle='-.')\n","_ = ax.set_ylim([0, 3500])\n","_ = ax.set_xlim([0, 250])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCzMYS3sF0wI"},"source":["df.sort_values(by='AvgSpeed', ascending=False).head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UB2X6VSltWHp"},"source":["We can see most of the values fall into a fairly reasonable range, but we have some outliers clustered around 200 that we'll need to fix or remove."]},{"cell_type":"markdown","metadata":{"id":"-BedMscEtWHr"},"source":["# Imputing missing values and fixing outliers"]},{"cell_type":"markdown","metadata":{"id":"jymqlrd6tWHs"},"source":["There are many ways to impute missing values, ranging from taking the average to building a model to predict the missing features. Let's first take a look at what percentage of rows have missing values for each of our numeric features."]},{"cell_type":"code","metadata":{"id":"-_2lh8nktWHt"},"source":["df.groupby('TripType')[['Distance', 'DayOfWeek', 'HourOfDay', 'AvgSpeed']].apply(lambda x: 100-x.notnull().sum()/len(x)*100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dRLYMIUbtWHw"},"source":["For our purposes, the number of rows with missing values is quite low (452 out of 15,895), and doesn't seem strongly biased to a particular trip type, so it should be safe to simply drop those rows which contain null values. What about the outliers? Let's define a speed outlier as an average trip speed exceeding 100km/hr."]},{"cell_type":"code","metadata":{"id":"dfv3dBNftWHx"},"source":["def gthan(row):\n","    return row>100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpwRWZDetWH0"},"source":["df.groupby(by='TripType')[['AvgSpeed']].apply(lambda x: gthan(x).sum()/len(x)*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtMWYjcntWH5"},"source":["df[df['AvgSpeed'] > 100].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-I0joIOtWIA"},"source":["There are only a few hundred rows with excessive speed outliers; we could probably safely drop them, but we should be mindful that the outlier speeds are imbalanced in our data by TripType. A larger percentage of them appear in highway driving. Instead of dropping them, let's replace them with the average for each type of trip; at the same time, we'll drop our null rows."]},{"cell_type":"code","metadata":{"id":"DsMlZ64qtWID"},"source":["trip_means = df[df['AvgSpeed'] < 100].groupby(by='TripType', as_index=True)[['AvgSpeed']].mean().rename(columns={'AvgSpeed': 'MeanSpeed'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtFHEHvMtWIH"},"source":["df_imputed = df.dropna().join(trip_means, on='TripType', how='left')\n","df_imputed['AvgSpeed'] = df_imputed.apply(lambda x: x['MeanSpeed'] if x['AvgSpeed']>100 else x['AvgSpeed'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQ9LyMonIbFD"},"source":["# SECOND PART"]},{"cell_type":"markdown","metadata":{"id":"k1RmYJcKtWIM"},"source":["# Building a model"]},{"cell_type":"markdown","metadata":{"id":"_efP8bLPtWIO"},"source":["To feed our features into a model, they must be made numeric. There are two main ways to encode categorical variables:\n","1. One Hot Encoding\n","2. Label Encoding\n","\n","One Hot Encoding turns each category into a new column with a binary indicator for whether the row belongs to that category. Label encoding creates an incrementing integer value to substitute for each category. One is not better than the other; each has its uses and drawbacks and you need to consider the type of algorithm used and how it will treat each of these types of encoding.\n","\n","Let's try both, on two types of models (logistic regression, and random forest) and see what happens."]},{"cell_type":"code","metadata":{"id":"aQUTbSGStWIQ"},"source":["dayofweek_ohe = OneHotEncoder().fit_transform(df_imputed['DayOfWeek'].values.reshape(-1, 1)).todense()\n","dayofweek_le = LabelEncoder().fit_transform(df_imputed['DayOfWeek'].values.reshape(-1, 1))[:, np.newaxis]\n","\n","# TODO: Apply one hot encoding and label encoding to the other appropriate feature columns \n","#\n","\n","distance = df_imputed['Distance'].values.reshape(-1, 1)\n","\n","# TODO: Apply reshaping to the other appropriate feature columns \n","#"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4YajQe1DMdT"},"source":["We chose some of the columns for scaling and some for one hot encoding. Do they all make sense to you? Do they match your choices? Feel free to change the concatenation code below to match your chioces. Let's keep going and re-evaluate later."]},{"cell_type":"code","metadata":{"id":"V3IV_h93tWIX"},"source":["X_ohe  = np.concatenate([dayofweek_ohe, triptype_ohe, hourofday_ohe, distance, speed], axis=1)\n","X_le  = np.concatenate([dayofweek_le, triptype_le, hourofday_le, distance, speed, driver_id, trip_num], axis=1)\n","y = label_binarize(df_imputed['Loaded'].values, classes=['Loaded', 'Unloaded'])\n","\n","X_test_ohe, X_train_ohe, y_test_ohe, y_train_ohe = train_test_split(X_ohe, y, shuffle=True)\n","X_test_le, X_train_le, y_test_le, y_train_le = train_test_split(X_le, y, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8WECklWhtWIa"},"source":["## Label Encoding"]},{"cell_type":"code","metadata":{"id":"qOEHpwUotWIb","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1578671430844,"user_tz":300,"elapsed":1356,"user":{"displayName":"Alessandro Bersia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDkCN4chtML2Qb9rSdZ1jkz8R55jRL3E0zW6cr5=s64","userId":"01884509692445689789"}},"outputId":"cedf7147-7e5c-4854-fd25-9e8430ae5e31"},"source":["logreg = LogisticRegression(penalty='l2')\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","_ = logreg.fit(X_train_le, y_train_le)\n","_ = rf.fit(X_train_le, y_train_le)\n","\n","print('Logistic Regression Accuracy: ', int(100*round(logreg.score(X_test_le, y_test_le), 2)), '%', sep='')\n","print('      Random Forest Accuracy: ', int(100*round(rf.score(X_test_le, y_test_le), 2)), '%', sep='')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Logistic Regression Accuracy: 61%\n","      Random Forest Accuracy: 71%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bapm75EStWIe"},"source":["The accuracy of both the Logistic Regression and Random Forest model are similar. Accuracy isn't the whole picture though; let's look at the F1 score and the ROC curve."]},{"cell_type":"code","metadata":{"id":"dedxNPONtWIf","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1578671436239,"user_tz":300,"elapsed":485,"user":{"displayName":"Alessandro Bersia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDkCN4chtML2Qb9rSdZ1jkz8R55jRL3E0zW6cr5=s64","userId":"01884509692445689789"}},"outputId":"d79c8079-b889-4ae8-87b9-10e87d90eee3"},"source":["print('Logistic Regression F1 Score: ', round(f1_score(logreg.predict(X_test_le), y_test_le), 2), sep='')\n","print('      Random Forest F1 Score: ', round(f1_score(rf.predict(X_test_le), y_test_le), 2), sep='')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Logistic Regression F1 Score: 0.67\n","      Random Forest F1 Score: 0.73\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zOm3841HtWIi"},"source":["y_score_lr = logreg.decision_function(X_test_le)\n","y_score_rf = rf.predict_proba(X_test_le)[:, 1]\n","\n","fpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","tpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","roc_auc = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","for i in range(1):\n","    fpr['lr'][i], tpr['lr'][i], _ = roc_curve(y_test_le, y_score_lr)\n","    roc_auc['lr'][i] = auc(fpr['lr'][i], tpr['lr'][i])\n","    fpr['rf'][i], tpr['rf'][i], _ = roc_curve(y_test_le, y_score_rf)\n","    roc_auc['rf'][i] = auc(fpr['rf'][i], tpr['rf'][i])\n","    \n","fig = plt.figure(figsize=(6, 6))\n","ax = fig.add_subplot(111)\n","lw = 2\n","_ = ax.plot(fpr['lr'][0], tpr['lr'][0], color='#00aeef',\n","         lw=lw, label='ROC Curve Logistic Regression (area = %0.2f)' % roc_auc['lr'][0], zorder=4)\n","_ = ax.plot(fpr['rf'][0], tpr['rf'][0], color='#93c83d',\n","         lw=lw, label='ROC Curve Random Forest (area = %0.2f)' % roc_auc['rf'][0], zorder=4)\n","_ = ax.plot([0, 1], [0, 1], color='#66788c', lw=lw, linestyle='--', zorder=3)\n","_ = ax.set_xlim([0.0, 1.0])\n","_ = ax.set_ylim([0.0, 1.0])\n","_ = ax.set_xlabel('False Positive Rate')\n","_ = ax.set_ylabel('True Positive Rate (recall)')\n","_ = ax.legend(loc=\"lower right\")\n","_ = ax.grid(linestyle='-.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AABqZXhjtWIm"},"source":["We can see from the ROC curve that the logistic regression is performing poorly compared to the random forest model. Why might that be? This is a case which demonstrates why you can't treat machine learning as a black box. Because of the way the algorithms work for the two models, label encoding categorical variables is poorly suited to use in logistic regression. Random forest models have multiple classifiers (it's an ensemble method, with many trees) and thus multiple hyperplane decision boundaries. Each label in a random forest model can thus have a decision associated with it. In logistic regression, you have only one hyperplane, which means that the order/value of the labels will be interpreted by the model as meaningful; in our case, they are not, so label encoding our categorical variables is a poor choice for logistic regression. Let's try one hot encoding instead."]},{"cell_type":"markdown","metadata":{"id":"z9aQoLABuGTX"},"source":["# FEATURE IMPORTANCE\n","One advantage of decision tree based algorithms is that we can compute the \"features importance\". It represents a function of the node impurity (e.g. Gini impurity or entropy) and the probability of reaching that node. As the name suggests, the higher the importance, the more important the feature. \n","Try plotting feature importance: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"]},{"cell_type":"markdown","metadata":{"id":"wvOnC4thDoDi"},"source":["Note: If you chose different columns/orders than those given above in our concatenation step, make sure to change the feature names list below to match."]},{"cell_type":"code","metadata":{"id":"6KhJkhq-t_ho"},"source":["features = ['DayOfWeek', 'TripType', 'HourOfDay', 'Distance', 'AvgSpeed', 'DriverId', 'TripNumber']\n","\n","ft_importance = rf.feature_importances_\n","\n","fi = pd.DataFrame({'Feature': features,\n","                   'importance':ft_importance}).\\\n","                    sort_values('importance', ascending=False)\n","\n","fi.plot(kind='barh', x='Feature', y='importance', figsize=(15,5), title='Feature Importance', color='#00aeef').invert_yaxis()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1jd4SYNKRyf"},"source":["Can you exclude features which are not significative for our model? Which ones?"]},{"cell_type":"markdown","metadata":{"id":"qS_bwRsxtWIn"},"source":["## One Hot Encoding"]},{"cell_type":"code","metadata":{"id":"i6m7--bftWIn"},"source":["logreg = LogisticRegression(penalty='l2')\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","_ = logreg.fit(X_train_ohe, y_train_ohe)\n","_ = rf.fit(X_train_ohe, y_train_ohe)\n","\n","print('Logistic Regression Accuracy: ', int(100*round(logreg.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')\n","print('      Random Forest Accuracy: ', int(100*round(rf.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q95cUcYStWIq"},"source":["The accuracy of both the Logistic Regression and Random Forest model are again similar, with logistic regression slightly improved. Accuracy isn't the whole picture though; let's look at the F1 score and the ROC curve again."]},{"cell_type":"code","metadata":{"id":"T4KO0keztWIr"},"source":["print('Logistic Regression F1 Score: ', round(f1_score(logreg.predict(X_test_ohe), y_test_ohe), 2), sep='')\n","print('      Random Forest F1 Score: ', round(f1_score(rf.predict(X_test_ohe), y_test_ohe), 2), sep='')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1q3c2VXhtWIt"},"source":["y_score_lr = logreg.decision_function(X_test_ohe)\n","y_score_rf = rf.predict_proba(X_test_ohe)[:, 1]\n","\n","fpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","tpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","roc_auc = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","for i in range(1):\n","    fpr['lr'][i], tpr['lr'][i], _ = roc_curve(y_test_ohe, y_score_lr)\n","    roc_auc['lr'][i] = auc(fpr['lr'][i], tpr['lr'][i])\n","    fpr['rf'][i], tpr['rf'][i], _ = roc_curve(y_test_ohe, y_score_rf)\n","    roc_auc['rf'][i] = auc(fpr['rf'][i], tpr['rf'][i])\n","    \n","fig = plt.figure(figsize=(6, 6))\n","ax = fig.add_subplot(111)\n","lw = 2\n","_ = ax.plot(fpr['lr'][0], tpr['lr'][0], color='#00aeef',\n","         lw=lw, label='ROC Curve Logistic Regression (area = %0.2f)' % roc_auc['lr'][0], zorder=4)\n","_ = ax.plot(fpr['rf'][0], tpr['rf'][0], color='#93c83d',\n","         lw=lw, label='ROC Curve Random Forest (area = %0.2f)' % roc_auc['rf'][0], zorder=4)\n","_ = ax.plot([0, 1], [0, 1], color='#66788c', lw=lw, linestyle='--', zorder=3)\n","_ = ax.set_xlim([0.0, 1.0])\n","_ = ax.set_ylim([0.0, 1.0])\n","_ = ax.set_xlabel('False Positive Rate')\n","_ = ax.set_ylabel('True Positive Rate (recall)')\n","_ = ax.legend(loc=\"lower right\")\n","_ = ax.grid(linestyle='-.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kkP9wWo5tWIw"},"source":["We see that with one hot encoding, both the logistic regression model and the random forest model are now performing similarly. One hot encoding is better suited to encoding for logistic regression."]},{"cell_type":"markdown","metadata":{"id":"R_CNVscRtWIx"},"source":["What about feature scaling? Because logistic regression has a single hyperplane decision boundary, features that have significant differences in magnitude can make it difficult for the algorithm to fit the plane. The differences here are small, so scaling likely will not have a big impact, but let's take a look. We'll use a min-max scaler and re-run our models."]},{"cell_type":"code","metadata":{"id":"eggzfxSKtWIy"},"source":["min_max_scaler = MinMaxScaler()\n","\n","X_test_ohe[:, X_test_ohe.shape[1]-2] = scale(X_test_ohe[:, X_test_ohe.shape[1]-2])\n","X_test_ohe[:, X_test_ohe.shape[1]-1] = scale(X_test_ohe[:, X_test_ohe.shape[1]-1])\n","X_train_ohe[:, X_train_ohe.shape[1]-2] = scale(X_train_ohe[:, X_train_ohe.shape[1]-2])\n","X_train_ohe[:, X_train_ohe.shape[1]-1] = scale(X_train_ohe[:, X_train_ohe.shape[1]-1])\n","X_test_ohe[:, X_test_ohe.shape[1]-2] = min_max_scaler.fit_transform(X_test_ohe[:, X_test_ohe.shape[1]-2])\n","X_test_ohe[:, X_test_ohe.shape[1]-1] = min_max_scaler.fit_transform(X_test_ohe[:, X_test_ohe.shape[1]-1])\n","X_train_ohe[:, X_train_ohe.shape[1]-2] = min_max_scaler.fit_transform(X_train_ohe[:, X_train_ohe.shape[1]-2])\n","X_train_ohe[:, X_train_ohe.shape[1]-1] = min_max_scaler.fit_transform(X_train_ohe[:, X_train_ohe.shape[1]-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cqJG0xYtWI1"},"source":["logreg = LogisticRegression(penalty='l2')\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","_ = logreg.fit(X_train_ohe, y_train_ohe)\n","_ = rf.fit(X_train_ohe, y_train_ohe)\n","\n","print('Logistic Regression Accuracy: ', int(100*round(logreg.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')\n","print('      Random Forest Accuracy: ', int(100*round(rf.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BA-EYDxptWI4"},"source":["y_score_lr = logreg.decision_function(X_test_ohe)\n","y_score_rf = rf.predict_proba(X_test_ohe)[:, 1]\n","\n","fpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","tpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","roc_auc = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","for i in range(1):\n","    fpr['lr'][i], tpr['lr'][i], _ = roc_curve(y_test_ohe, y_score_lr)\n","    roc_auc['lr'][i] = auc(fpr['lr'][i], tpr['lr'][i])\n","    fpr['rf'][i], tpr['rf'][i], _ = roc_curve(y_test_ohe, y_score_rf)\n","    roc_auc['rf'][i] = auc(fpr['rf'][i], tpr['rf'][i])\n","    \n","fig = plt.figure(figsize=(6, 6))\n","ax = fig.add_subplot(111)\n","lw = 2\n","_ = ax.plot(fpr['lr'][0], tpr['lr'][0], color='#00aeef',\n","         lw=lw, label='ROC Curve Logistic Regression (area = %0.2f)' % roc_auc['lr'][0], zorder=4)\n","_ = ax.plot(fpr['rf'][0], tpr['rf'][0], color='#93c83d',\n","         lw=lw, label='ROC Curve Random Forest (area = %0.2f)' % roc_auc['rf'][0], zorder=4)\n","_ = ax.plot([0, 1], [0, 1], color='#66788c', lw=lw, linestyle='--', zorder=3)\n","_ = ax.set_xlim([0.0, 1.0])\n","_ = ax.set_ylim([0.0, 1.0])\n","_ = ax.set_xlabel('False Positive Rate')\n","_ = ax.set_ylabel('True Positive Rate (recall)')\n","_ = ax.legend(loc=\"lower right\")\n","_ = ax.grid(linestyle='-.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"niElsUJDtWI7"},"source":["The results are very similar to before. But what would happen if we had another feature with much larger units? Let's say the distance was for some reason in centimeters instead of kilometers, and we didn't do any scaling."]},{"cell_type":"code","metadata":{"id":"YSsTYi0MtWJB"},"source":["dayofweek_ohe = OneHotEncoder().fit_transform(df_imputed['DayOfWeek'].values.reshape(-1, 1)).todense()\n","triptype_ohe = OneHotEncoder().fit_transform(df_imputed['TripType'].values.reshape(-1, 1)).todense()\n","hourofday_ohe = OneHotEncoder().fit_transform(df_imputed['HourOfDay'].values.reshape(-1, 1)).todense()\n","\n","distance = df_imputed['Distance'].values.reshape(-1, 1)*100000\n","speed = df_imputed['AvgSpeed'].values.reshape(-1, 1)\n","\n","X_ohe  = np.concatenate([dayofweek_ohe, triptype_ohe, hourofday_ohe, distance, speed], axis=1)\n","y = label_binarize(df_imputed['Loaded'].values, classes=['Loaded', 'Unloaded'])\n","\n","X_test_ohe, X_train_ohe, y_test_ohe, y_train_ohe = train_test_split(X_ohe, y, shuffle=True)\n","\n","logreg = LogisticRegression(penalty='l2')\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","_ = logreg.fit(X_train_ohe, y_train_ohe)\n","_ = rf.fit(X_train_ohe, y_train_ohe)\n","\n","print('Logistic Regression Accuracy: ', int(100*round(logreg.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')\n","print('      Random Forest Accuracy: ', int(100*round(rf.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCdbKhevtWJG"},"source":["Our logistic regression accuracy has dropped down to 55%, while the random forest accuracy is relatively unchanged. Let's take a look at the ROC curve."]},{"cell_type":"code","metadata":{"id":"YVZnSqvotWJH"},"source":["y_score_lr = logreg.decision_function(X_test_ohe)\n","y_score_rf = rf.predict_proba(X_test_ohe)[:, 1]\n","\n","fpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","tpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","roc_auc = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","for i in range(1):\n","    fpr['lr'][i], tpr['lr'][i], _ = roc_curve(y_test_ohe, y_score_lr)\n","    roc_auc['lr'][i] = auc(fpr['lr'][i], tpr['lr'][i])\n","    fpr['rf'][i], tpr['rf'][i], _ = roc_curve(y_test_ohe, y_score_rf)\n","    roc_auc['rf'][i] = auc(fpr['rf'][i], tpr['rf'][i])\n","    \n","fig = plt.figure(figsize=(6, 6))\n","ax = fig.add_subplot(111)\n","lw = 2\n","_ = ax.plot(fpr['lr'][0], tpr['lr'][0], color='#00aeef',\n","         lw=lw, label='ROC Curve Logistic Regression (area = %0.2f)' % roc_auc['lr'][0], zorder=4)\n","_ = ax.plot(fpr['rf'][0], tpr['rf'][0], color='#93c83d',\n","         lw=lw, label='ROC Curve Random Forest (area = %0.2f)' % roc_auc['rf'][0], zorder=4)\n","_ = ax.plot([0, 1], [0, 1], color='#66788c', lw=lw, linestyle='--', zorder=3)\n","_ = ax.set_xlim([0.0, 1.0])\n","_ = ax.set_ylim([0.0, 1.0])\n","_ = ax.set_xlabel('False Positive Rate')\n","_ = ax.set_ylabel('True Positive Rate (recall)')\n","_ = ax.legend(loc=\"lower right\")\n","_ = ax.grid(linestyle='-.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M01IoLTytWJK"},"source":["We can see from the ROC curve that the logistic regression model is now performing worse than random chance, due to the inappropriate scaling. Let's scale our inputs and try again."]},{"cell_type":"code","metadata":{"id":"48mRzS31tWJL"},"source":["min_max_scaler = MinMaxScaler()\n","\n","X_test_ohe[:, X_test_ohe.shape[1]-2] = scale(X_test_ohe[:, X_test_ohe.shape[1]-2])\n","X_test_ohe[:, X_test_ohe.shape[1]-1] = scale(X_test_ohe[:, X_test_ohe.shape[1]-1])\n","X_train_ohe[:, X_train_ohe.shape[1]-2] = scale(X_train_ohe[:, X_train_ohe.shape[1]-2])\n","X_train_ohe[:, X_train_ohe.shape[1]-1] = scale(X_train_ohe[:, X_train_ohe.shape[1]-1])\n","X_test_ohe[:, X_test_ohe.shape[1]-2] = min_max_scaler.fit_transform(X_test_ohe[:, X_test_ohe.shape[1]-2])\n","X_test_ohe[:, X_test_ohe.shape[1]-1] = min_max_scaler.fit_transform(X_test_ohe[:, X_test_ohe.shape[1]-1])\n","X_train_ohe[:, X_train_ohe.shape[1]-2] = min_max_scaler.fit_transform(X_train_ohe[:, X_train_ohe.shape[1]-2])\n","X_train_ohe[:, X_train_ohe.shape[1]-1] = min_max_scaler.fit_transform(X_train_ohe[:, X_train_ohe.shape[1]-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_Uleu-HtWJN"},"source":["logreg = LogisticRegression(penalty='l2')\n","rf = RandomForestClassifier(n_estimators=100)\n","\n","_ = logreg.fit(X_train_ohe, y_train_ohe)\n","_ = rf.fit(X_train_ohe, y_train_ohe)\n","\n","print('Logistic Regression Accuracy: ', int(100*round(logreg.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')\n","print('      Random Forest Accuracy: ', int(100*round(rf.score(X_test_ohe, y_test_ohe), 2)), '%', sep='')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLol4ZZTtWJQ"},"source":["y_score_lr = logreg.decision_function(X_test_ohe)\n","y_score_rf = rf.predict_proba(X_test_ohe)[:, 1]\n","\n","fpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","tpr = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","roc_auc = {'lr': {0: 0, 1: 0}, 'rf': {0: 0, 1: 0}}\n","for i in range(1):\n","    fpr['lr'][i], tpr['lr'][i], _ = roc_curve(y_test_ohe, y_score_lr)\n","    roc_auc['lr'][i] = auc(fpr['lr'][i], tpr['lr'][i])\n","    fpr['rf'][i], tpr['rf'][i], _ = roc_curve(y_test_ohe, y_score_rf)\n","    roc_auc['rf'][i] = auc(fpr['rf'][i], tpr['rf'][i])\n","    \n","fig = plt.figure(figsize=(6, 6))\n","ax = fig.add_subplot(111)\n","lw = 2\n","_ = ax.plot(fpr['lr'][0], tpr['lr'][0], color='#00aeef',\n","         lw=lw, label='ROC Curve Logistic Regression (area = %0.2f)' % roc_auc['lr'][0], zorder=4)\n","_ = ax.plot(fpr['rf'][0], tpr['rf'][0], color='#93c83d',\n","         lw=lw, label='ROC Curve Random Forest (area = %0.2f)' % roc_auc['rf'][0], zorder=4)\n","_ = ax.plot([0, 1], [0, 1], color='#66788c', lw=lw, linestyle='--', zorder=3)\n","_ = ax.set_xlim([0.0, 1.0])\n","_ = ax.set_ylim([0.0, 1.0])\n","_ = ax.set_xlabel('False Positive Rate')\n","_ = ax.set_ylabel('True Positive Rate (recall)')\n","_ = ax.legend(loc=\"lower right\")\n","_ = ax.grid(linestyle='-.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SjxllPpktWJT"},"source":["We're back to normal again! You can see the importance of understanding the underlying algorithms and how they treat your features."]}]}