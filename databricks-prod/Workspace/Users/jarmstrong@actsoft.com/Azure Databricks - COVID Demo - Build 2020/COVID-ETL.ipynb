{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, TimestampType\nschema = StructType([\n  StructField('FIPS', IntegerType(), True), \n  StructField('Admin2', StringType(), True),\n  StructField('Province_State', StringType(), True),  \n  StructField('Country_Region', StringType(), True),  \n  StructField('Last_Update', TimestampType(), True),  \n  StructField('Lat', DoubleType(), True),  \n  StructField('Long_', DoubleType(), True),\n  StructField('Confirmed', IntegerType(), True), \n  StructField('Deaths', IntegerType(), True), \n  StructField('Recovered', IntegerType(), True), \n  StructField('Active', IntegerType(), True),   \n  StructField('Combined_Key', StringType(), True),  \n  StructField('process_date', DateType(), True),    \n])\n\n# Create initial empty Spark DataFrame based on preceding schema\njhu_daily = spark.createDataFrame([], schema)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import os\nimport pandas as pd\nimport glob\nfrom pyspark.sql.functions import input_file_name, lit, col\n\n# Creates a list of all csv files\nglobbed_files = glob.glob(\"/dbfs/databricks-datasets/COVID/CSSEGISandData/csse_covid_19_data/csse_covid_19_daily_reports/*.csv\") \n#globbed_files = glob.glob(\"/dbfs/databricks-datasets/COVID/CSSEGISandData/csse_covid_19_data/csse_covid_19_daily_reports/04*.csv\")\n\ni = 0\nfor csv in globbed_files:\n  # Filename\n  source_file = csv[5:200]\n  process_date = csv[100:104] + \"-\" + csv[94:96] + \"-\" + csv[97:99]\n  \n  # Read data into temporary dataframe\n  df_tmp = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(source_file)\n  df_tmp.createOrReplaceTempView(\"df_tmp\")\n\n  # Obtain schema\n  schema_txt = ' '.join(map(str, df_tmp.columns)) \n  \n  # Three schema types (as of 2020-04-08) \n  schema_01 = \"Province/State Country/Region Last Update Confirmed Deaths Recovered\" # 01-22-2020 to 02-29-2020\n  schema_02 = \"Province/State Country/Region Last Update Confirmed Deaths Recovered Latitude Longitude\" # 03-01-2020 to 03-21-2020\n  schema_03 = \"FIPS Admin2 Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active Combined_Key\" # 03-22-2020 to\n  \n  # Insert data based on schema type\n  if (schema_txt == schema_01):\n    df_tmp = (df_tmp\n                .withColumn(\"FIPS\", lit(None).cast(IntegerType()))\n                .withColumn(\"Admin2\", lit(None).cast(StringType()))\n                .withColumn(\"Province_State\", col(\"Province/State\"))\n                .withColumn(\"Country_Region\", col(\"Country/Region\"))\n                .withColumn(\"Last_Update\", col(\"Last Update\"))\n                .withColumn(\"Lat\", lit(None).cast(DoubleType()))\n                .withColumn(\"Long_\", lit(None).cast(DoubleType()))\n                .withColumn(\"Active\", lit(None).cast(IntegerType()))\n                .withColumn(\"Combined_Key\", lit(None).cast(StringType()))\n                .withColumn(\"process_date\", lit(process_date))\n                .select(\"FIPS\", \n                        \"Admin2\", \n                        \"Province_State\", \n                        \"Country_Region\", \n                        \"Last_Update\", \n                        \"Lat\", \n                        \"Long_\", \n                        \"Confirmed\", \n                        \"Deaths\", \n                        \"Recovered\", \n                        \"Active\", \n                        \"Combined_Key\", \n                        \"process_date\")\n               )\n    jhu_daily = jhu_daily.union(df_tmp)\n  elif (schema_txt == schema_02):\n    df_tmp = (df_tmp\n                .withColumn(\"FIPS\", lit(None).cast(IntegerType()))\n                .withColumn(\"Admin2\", lit(None).cast(StringType()))\n                .withColumn(\"Province_State\", col(\"Province/State\"))\n                .withColumn(\"Country_Region\", col(\"Country/Region\"))\n                .withColumn(\"Last_Update\", col(\"Last Update\"))\n                .withColumn(\"Lat\", col(\"Latitude\"))\n                .withColumn(\"Long_\", col(\"Longitude\"))\n                .withColumn(\"Active\", lit(None).cast(IntegerType()))\n                .withColumn(\"Combined_Key\", lit(None).cast(StringType()))\n                .withColumn(\"process_date\", lit(process_date))\n                .select(\"FIPS\", \n                        \"Admin2\", \n                        \"Province_State\", \n                        \"Country_Region\", \n                        \"Last_Update\", \n                        \"Lat\", \n                        \"Long_\", \n                        \"Confirmed\", \n                        \"Deaths\", \n                        \"Recovered\", \n                        \"Active\", \n                        \"Combined_Key\", \n                        \"process_date\")\n               )\n    jhu_daily = jhu_daily.union(df_tmp)\n\n  elif (schema_txt == schema_03):\n    df_tmp = df_tmp.withColumn(\"process_date\", lit(process_date))\n    jhu_daily = jhu_daily.union(df_tmp)\n  else:\n    print(\"Schema may have changed\")\n    raise\n  \n  # print out the schema being processed by date\n  print(\"%s | %s\" % (process_date, schema_txt))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["jhu_daily.createOrReplaceTempView(\"jhu_daily_covid\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sh mkdir -p /dbfs/tmp/kyweller/COVID/population_estimates_by_county/ && wget -O /dbfs/tmp/kyweller/COVID/population_estimates_by_county/co-est2019-alldata.csv https://raw.githubusercontent.com/databricks/tech-talks/master/datasets/co-est2019-alldata.csv && ls -al /dbfs/tmp/kyweller/COVID/population_estimates_by_county/"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["map_popest_county = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/kyweller/COVID/population_estimates_by_county/co-est2019-alldata.csv\")\nmap_popest_county.createOrReplaceTempView(\"map_popest_county\")\nfips_popest_county = spark.sql(\"select State * 1000 + substring(cast(1000 + County as string), 2, 3) as fips, STNAME, CTYNAME, census2010pop, POPESTIMATE2019 from map_popest_county\")\nfips_popest_county.createOrReplaceTempView(\"fips_popest_county\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["jhu_daily_pop = spark.sql(\"\"\"\nSELECT f.FIPS, f.Admin2, f.Province_State, f.Country_Region, f.Last_Update, f.Lat, f.Long_, f.Confirmed, f.Deaths, f.Recovered, f.Active, f.Combined_Key, f.process_date, p.POPESTIMATE2019 \n  FROM jhu_daily_covid f\n    JOIN fips_popest_county p\n      ON p.fips = f.FIPS\n\"\"\")\njhu_daily_pop.createOrReplaceTempView(\"jhu_daily_pop\")"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"COVID-ETL","notebookId":537053288452024},"nbformat":4,"nbformat_minor":0}