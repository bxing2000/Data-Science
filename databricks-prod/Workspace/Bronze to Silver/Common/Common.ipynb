{"cells":[{"cell_type":"code","source":["%run \"../Common/Enums\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# from pyspark.sql.functions import lit \nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.dataframe import DataFrame\n\ndef AppendCustomColumns(\n  dataframe,\n  customColumns: list = []\n):\n  print('Adding custom columns...')  \n  \n  if not customColumns:\n    print('There is no custom columns provided.')\n    \n  outcols = dataframe.columns\n  for column in customColumns:\n    if not column in outcols:\n      print(f'Custom column added: {column}')\n      outcols.append(F.lit(None).cast(StringType()).alias('{0}'.format(column)))\n  \n  return dataframe.select(outcols)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from datetime import datetime\nfrom enum import Enum\n\ndef PrepareDataBronze( \n      lakeName,    \n      instance: Enum, \n      entity: BronzeTable,\n      entityName,\n      startDate: datetime, #date from which data will be read  \n      customColumns: list = []\n    ):\n  \n  fileSystem = 'bronze'\n  \n  dataSource = entity.value[TableParams.DataSource] \n  tableName = entity.value[TableParams.TableName] \n  pkColumns = entity.value[TableParams.PkColumns] \n  modificationTimeColumn = entity.value[TableParams.ModificationTimeColumn] \n  \n  year = str(startDate.year)\n  month = str(startDate.month)\n  day = str(startDate.day)\n  \n  startDateStr = str(startDate)\n  \n  #reading only requred partitions starting from startDate\n  rawData = spark.read \\\n    .format('parquet') \\\n    .load(\"wasbs://\"+ fileSystem +\"@\"+ lakeName +\".blob.core.windows.net/\"+ dataSource +\"/\"+ instance.name +\"/\"+tableName) \\\n    .where(\"\"\"\n             ( Year > \"\"\" + year + \"\"\" )\n          or ( Year = \"\"\" + year + \"\"\" and Month > \"\"\" + month + \"\"\" )\n          or ( Year = \"\"\" + year + \"\"\" and Month = \"\"\" + month + \"\"\" and Day >= \"\"\" + day + \"\"\" )\n    \"\"\") \\\n    .where(modificationTimeColumn + ' > \"' + startDateStr.replace('T',' ') + '\"') # probably need to change to startDate obj variable.\n    \n  # add aditional filter by partition column modificationTimeColumn >= startDateStr\n  \n  rawData = AppendCustomColumns(rawData, customColumns)\n  rawData.registerTempTable(\"Raw\"+entityName)\n  \n  #removing duplicates from rawData dataset\n  data = spark.sql(\"\"\" \n    with cteData\n    as (\n      select \n          row_number() over(partition by \"\"\" + pkColumns + \" order by \" + modificationTimeColumn + \"\"\" desc) as RowNum\n        , *\n      from raw\"\"\"+entityName+\"\"\"\n    )\n    select * \n    from cteData\n    where RowNum = 1\n  \"\"\");\n\n  data.registerTempTable(entityName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["\ndef PrepareDataBronzeWithoutInstance( \n      lakeName,    \n      entity: BronzeTable,\n      entityName,\n      startDate: datetime, #date from which data will be read \n      customColumns: list = []\n    ):\n  \n  fileSystem = 'bronze'\n  \n  dataSource = entity.value[TableParams.DataSource] \n  tableName = entity.value[TableParams.TableName] \n  pkColumns = entity.value[TableParams.PkColumns] \n  modificationTimeColumn = entity.value[TableParams.ModificationTimeColumn]\n  \n  year = str(startDate.year)\n  month = str(startDate.month)\n  day = str(startDate.day)\n  \n  startDateStr = str(startDate)\n  \n  #reading only requred partitions starting from startDate\n  rawData = spark.read \\\n    .format('parquet') \\\n    .load(\"wasbs://\"+ fileSystem +\"@\"+ lakeName +\".blob.core.windows.net/\"+ dataSource +\"/\"+ tableName) \\\n    .where(\"\"\"\n             ( Year > \"\"\" + year + \"\"\" )\n          or ( Year = \"\"\" + year + \"\"\" and Month > \"\"\" + month + \"\"\" )\n          or ( Year = \"\"\" + year + \"\"\" and Month = \"\"\" + month + \"\"\" and Day >= \"\"\" + day + \"\"\" )\n    \"\"\") \\\n    .where(modificationTimeColumn + ' > \"' + startDateStr.replace('T',' ') + '\"') # probably need to change to startDate obj variable.\n    \n  # add aditional filter by partition column modificationTimeColumn >= startDateStr\n            \n  rawData = AppendCustomColumns(rawData, customColumns)\n  rawData.registerTempTable(\"Raw\"+entityName)\n  \n  #removing duplicates from rawData dataset\n  data = spark.sql(f\"\"\" \n    with cteData\n    as (\n      select \n          row_number() over(partition by {pkColumns} order by {modificationTimeColumn} desc) as RowNum\n        , *\n      from raw{entityName}\n    )\n    select * \n    from cteData\n    where RowNum = 1\n  \"\"\");\n\n  data.registerTempTable(entityName)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def PrepareDataBronzeSimple( \n      lakeName, \n      instance: Enum, \n      entity: BronzeTable, \n      entityName, \n      customColumns: list = [] \n    ) :\n  fileSystem = 'bronze'\n  \n  dataSource = entity.value[TableParams.DataSource] \n  tableName = entity.value[TableParams.TableName] \n  \n  data = spark.read.format('parquet').load(\"wasbs://\"+ fileSystem +\"@\"+ lakeName +\".blob.core.windows.net/\"+ dataSource + \"/\" + instance.name + \"/\" + tableName)\n  data = AppendCustomColumns(data, customColumns)\n  data.registerTempTable(entityName)\n  "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["def PrepareDataBronzeSimpleWithoutInstance( \n      lakeName, \n      entity: BronzeTable, \n      entityName,\n      customColumns: list = []\n  ) :\n  fileSystem = 'bronze'\n  \n  dataSource = entity.value[TableParams.DataSource] \n  tableName = entity.value[TableParams.TableName] \n  \n  data = spark.read.format('parquet').load(\"wasbs://\"+ fileSystem +\"@\"+ lakeName +\".blob.core.windows.net/\"+ dataSource + \"/\" + tableName)\n  data = AppendCustomColumns(data, customColumns)\n  data.registerTempTable(entityName)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# def GetLastSyncDate(Entity, Source):\n#   FileName = \"wasbs://system@\"+ lakeName +\".blob.core.windows.net/\"+ Entity + \"/\" + Entity + \"_\" + Source + \".json\"\n#   try:\n#     df = spark.read.format('json').load(FileName)    \n#     LastSyncDate = str(df.first().LastSyncDate).replace(' ','T')\n#     if LastSyncDate.count('.')==0:\n#       LastSyncDate += \".0\"\n    \n# #     print(df)\n# #     print(str(df.first().LastSyncDate))\n# #     print(LastSyncDate)\n#   except:\n#       LastSyncDate = \"2000-01-01T00:00:00.0\"\n#   return LastSyncDate"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["def GetLastSyncDate(Entity, Source):\n  FileName = \"wasbs://system@\"+ lakeName +\".blob.core.windows.net/silver/\"+ Entity + \"/\" + Entity + \"_\" + Source + \".json\"\n  try:\n    df = spark.read.format('json').load(FileName)    \n    LastSyncDate = str(df.first().LastSyncDate).replace(' ','T')\n    if LastSyncDate.count('.')==0:\n      LastSyncDate += \".0\"\n\n    LastSyncDate = datetime.strptime(LastSyncDate,'%Y-%m-%dT%H:%M:%S.%f')\n    \n  except:\n      LastSyncDate = datetime.strptime(\"2000-01-01T00:00:00.0\",'%Y-%m-%dT%H:%M:%S.%f')\n  \n  return LastSyncDate"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def UpdateLastSyncDate(LastSyncDate, Entity, Source):\n  LastSyncDate = str(LastSyncDate)\n  if LastSyncDate.count('.')==0:\n    LastSyncDate += \".0\"\n  Content = '{\"LastSyncDate\":\"'+LastSyncDate.replace(\" \",\"T\")+'\"}'\n  FileName = \"wasbs://system@\"+ lakeName +\".blob.core.windows.net/silver/\"+ Entity + \"/\" + Entity + \"_\" + Source + \".json\"\n  print(\"Save \", Content,sep=\"\")\n  dbutils.fs.put(\n    file = FileName, \n    contents = Content,\n    overwrite = True\n  )"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Active'\n      union all select 1,\t'Inactive'\n      union all select 2,\t'Suspended'\n      union all select 3,\t'OperationalPending'\n      union all select 4,\t'Deleted'\n      union all select 100,\t'Initializing'\n      union all select 200,\t'Migrating'\n      union all select 201,\t'Migrated'\n    ) t ( CompanyStatusId, CompanyStatusName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpCompanyStatuses\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 1, 'Data Collection > Build New Module'\n      union all select 2, 'Data Collection > Edit Module'\n      union all select 3, 'Data Collection > Edit Charts/Dashboard'\n      union all select 4, 'Scheduled Reports'\n      union all select 5, 'Dispatching > Orders Screen Filter Infographics'\n      union all select 6, 'Dispatching > Orders Screen User List/Drag and Drop'\n      union all select 7, 'Dispatching > Edit Dispatch Module'\n      union all select 8, 'Timekeeping > Create Clock In/Out Forms'\n      union all select 9, 'Auto Email Conditional'\n      union all select 10, 'Tracking Included by default'\n      union all select 11, 'Enable Groups'\n      union all select 12, 'Dispatching > Add Dispatch Module From Library'\n      union all select 13, 'Add Messaging Module'\n      union all select 14, 'Enable Push-to-talk'\n      union all select 15, 'Data Collection > Edit Submitted Data'\n      union all select 16, 'Enable File import'\n      union all select 17, 'Enable Ad hoc reports'\n      union all select 18, 'Timekeeping > Add Timekeeping'\n      union all select 19, 'VIN Lookup'\n      union all select 20, 'Edit Submitted Data'\n      union all select 21, 'API Access'\n      union all select 22, 'Data Collection > Form Routing'\n      union all select 23, 'Data Collection > Form Sharing'\n      union all select 24, 'Emails > Standalone forms'\n      union all select 25, 'Emails > Dispatching orders'\n      union all select 26, 'Emails > Alerts'\n      union all select 27, 'Data Collection > GetFormDefinitions'\n      union all select 28, 'Data Collection > Edit address'\n      union all select 29, 'Custom tracking interval'\n      union all select 30, 'Dispatching > Scheduler'\n      union all select 31, 'Custom lists'\n    ) t ( FeatureId, FeatureName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpFeatures\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Handsets'\n      union all select 1,\t'CalAmp'\n      union all select 2,\t'GeoTab'\n      union all select 3,\t'Xirgo'\n      union all select 4,\t'Suntech'\n    ) t ( DeviceTypeId, DeviceTypeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpDeviceTypes\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Completed'\n      union all select 1,\t'Assigned'\n      union all select 2,\t'Unassigned'\n      union all select 3,\t'Declined'\n      union all select 4,\t'Cancelled'\n    ) t ( StandaloneFormStatusId, StandaloneFormStatusName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpStandaloneFormStatus\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 1,\t'ClockIn'\n      union all select 2,\t'Break'\n      union all select 3,\t'Lunch'\n    ) t ( TimekeepingStatusId, TimekeepingStatusName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpTimekeepingStatusName\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n    select *\n    from(\n                select 1, 'Create'\n      union all select 2, 'Update'\n      union all select 3, 'Delete'\n      union all select 4, 'Access'\n  )\n  t ( OperationId, OperationName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpAuditOperation\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n    select *\n    from(\n                select 0, 'Unknown'\n      union all select 1, 'Create'\n      union all select 2, 'Update'\n      union all select 3, 'Delete'\n      union all select 4, 'Activate'\n      union all select 5, 'Deactivate'\n      union all select 11, 'Cascade'\n      union all select 12, 'Merge'\n      union all select 13, 'Assign'\n      union all select 14, 'Share'\n      union all select 15, 'Retrieve' \n      union all select 16, 'Close'\n      union all select 17, 'Cancel'\n      union all select 18, 'Complete'\n      union all select 20, 'Resolve'\n      union all select 21, 'Reopen'\n      union all select 22, 'Fulfill'\n      union all select 23, 'Paid'\n      union all select 24, 'Qualify'\n      union all select 25, 'Disqualify'\n      union all select 26, 'Submit'\n      union all select 27, 'Reject'\n      union all select 28, 'Approve'\n      union all select 29, 'Invoice'\n      union all select 30, 'Hold'\n      union all select 31, 'Add Member'\n      union all select 32, 'Remove Member'\n      union all select 33, 'Associate Entities'\n      union all select 34, 'Disassociate Entities'\n      union all select 35, 'Add Members'\n      union all select 36, 'Remove Members'\n      union all select 37, 'Add Item'\n      union all select 38, 'Remove Item'\n      union all select 39, 'Add Substitute'\n      union all select 40, 'Remove Substitute'\n      union all select 41, 'Set State'\n      union all select 42, 'Renew'\n      union all select 43, 'Revise'\n      union all select 44, 'Win'\n      union all select 45, 'Lose'\n      union all select 46, 'Internal Processing'\n      union all select 47, 'Reschedule'\n      union all select 48, 'Modify Share'\n      union all select 49, 'Unshare'\n      union all select 50, 'Book'\n      union all select 51, 'Generate Quote From Opportunity'\n      union all select 52, 'Add To Queue'\n      union all select 53, 'Assign Role To Team'\n      union all select 54, 'Remove Role From Team'\n      union all select 55, 'Assign Role To User'\n      union all select 56, 'Remove Role From User'\n      union all select 57, 'Add Privileges to Role'\n      union all select 58, 'Remove Privileges From Role'\n      union all select 59, 'Replace Privileges In Role'\n      union all select 60, 'Import Mappings'\n      union all select 61, 'Clone'\n      union all select 62, 'Send Direct Email'\n      union all select 63, 'Enabled for organization'\n      union all select 64, 'User Access via Web'\n      union all select 65, 'User Access via Web Services'\n      union all select 100, 'Delete Entity'\n      union all select 101, 'Delete Attribute'\n      union all select 102, 'Audit Change at Entity Level'\n      union all select 103, 'Audit Change at Attribute Level'\n      union all select 104, 'Audit Change at Org Level'\n      union all select 105, 'Entity Audit Started'\n      union all select 106, 'Attribute Audit Started'\n      union all select 107, 'Audit Enabled'\n      union all select 108, 'Entity Audit Stopped'\n      union all select 109, 'Attribute Audit Stopped'\n      union all select 110, 'Audit Disabled'\n      union all select 111, 'Audit Log Deletion'\n      union all select 112, 'User Access Audit Started'\n      union all select 113, 'User Access Audit Stopped'\n  ) t ( ActionId, ActionName )\n  \"\"\");\n  \ndata.registerTempTable(\"tmpAuditAction\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 1,\t'Administrator'\n      union all select 3,\t'User'\n      union all select 5,\t'Power User'\n    ) t ( UserRoleId, UserRoleName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpUserRole\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Undefined'\n      union all select 1,\t'Regular'\n      union all select 2,\t'WebOnly'\n      union all select 3,\t'Employee'\n    ) t ( UserLicenseTypeId, UserLicenseTypeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpUserLicenseType\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Active'\n      union all select 1,\t'Resolved'\n      union all select 2,\t'Canceled'\n    ) t ( StateCode, StateCodeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpIncidentStateCode\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 1,\t'In Progress'\n      union all select 2,\t'On Hold'\n      union all select 3,\t'Waiting for Details'\n      union all select 4,\t'Researching'\n      union all select 5,\t'Problem Solved'\n      union all select 1000,\t'Information Provided'\n      union all select 100000000, 'Non-Responsive'\n      union all select 6, 'Canceled'\n      union all select 2000, 'Merged'\n    ) t ( StatusCode, StatusCodeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpIncidentStatusCode\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Open'\n      union all select 1,\t'Won'\n      union all select 2,\t'Lost'\n    ) t ( StateCode, StateCodeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpOpportunityStateCode\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 1,\t'In Progress'\n      union all select 2,\t'On Hold'\n      union all select 3,\t'Won'\n      union all select 100000001,\t'Missing Information'\n      union all select 100000004,\t'Duplicate'\n      union all select 100000000,\t'Competitor'\n      union all select 4, 'Canceled'\n      union all select 5, 'Out-Sold'\n      union all select 100000002, 'Deact User/users'\n      union all select 100000003, 'Deact Account'\n      union all select 100000005, 'User Already Active'\n      union all select 100000006, 'Billing Not Verified'\n      union all select 100000007, 'Account Suspended'\n    ) t ( StatusCode, StatusCodeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpOpportunityStatusCode\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Active'\n      union all select 1,\t'Inactive'\n      union all select 2,\t'Suspended'\n    ) t ( LicenseStatusId, LicenseStatusName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpLicenseStatus\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0, 'Unexpected'\n      union all select 1, 'MultipleLicensesWithPtn'\n      union all select 2, 'LimitedWhenCarrierReceived'\n      union all select 3, 'ChangeTier'\n      union all select 4, 'ChangeBan'\n      union all select 5, 'ChangePtn'\n      union all select 6, 'ReplaceLimited'\n      union all select 7, 'CarrierCancel'\n      union all select 8, 'UserCancel'\n      union all select 9, 'DeactivateOldPromo'\n      union all select 10, 'ChangeBanAndPtn'\n      union all select 11, 'NoActiveNonPromoLicenses'\n      union all select 12, 'Expired'\n      union all select 13, 'CompanyAbandoned'\n      union all select 14, 'Suspended'\n      union all select 15, 'SuspendedCompanyDeactivated'\n      union all select 16, 'CompanyMerged'\n      union all select 17, 'CompanyCreationFailed'\n      union all select 18, 'ChangeLicenseParams'\n      union all select 19, 'CarrierTierChange'\n      union all select 20, 'CarrierPriceLevelChange'\n      union all select 21, 'CarrierReactivation'\n      union all select 22, 'Transfer'\n    ) t ( LicenseDeactivationReasonId, LicenseDeactivationReasonName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpLicenseDeactivationReason\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Unexpected'\n      union all select 1,\t'CarrierInitiated'\n      union all select 2,\t'ChangeTier'\n      union all select 3,\t'LicenseChangeRequest'\n      union all select 4,\t'ReplaceLimited'\n      union all select 5,\t'ChangeBan'\n      union all select 6,\t'ChangePtn'\n      union all select 7,\t'Renew'\n      union all select 8,\t'CarrierNotification'\n      union all select 9,\t'DeactivateOldPromo'\n      union all select 10,\t'AddPromo'\n      union all select 11,\t'ChangeBanAndPtn'\n      union all select 12,\t'AppDirectNotification'\n      union all select 13,\t'OrphanModify'\n      union all select 14,\t'CompanyMerged'\n      union all select 15,\t'CarrierTierChange'\n      union all select 16,\t'CarrierPriceLevelChange'\n      union all select 17,\t'CarrierReactivation'\n      union all select 18,\t'LicenseTransfer'\n      union all select 19,\t'ResumeNotification'\n      union all select 20,\t'CompanyMigration'\n    ) t ( LicenseActivationReasonId, LicenseActivationReasonName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpLicenseActivationReason\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'New'\n      union all select 1,\t'Dispatched'\n      union all select 2,\t'InProgress'\n      union all select 3,\t'Complete'\n      union all select 4,\t'Cancelled'\n    ) t ( OrderStatusTypeId, OrderStatusType )\n  \"\"\");\n\ndata.registerTempTable(\"tmpOrderStatusTypes\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["data = spark.sql(\"\"\" \n    select *\n    from(\n                select 0,\t'Circle'\n      union all select 1,\t'Polygon'\n    ) t ( ShapeTypeId, ShapeTypeName )\n  \"\"\");\n\ndata.registerTempTable(\"tmpShapeTypes\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n  select * from (\n              select 1, 'Phone'\n    union all select 2, 'Email'\n    union all select 3, 'Web'\n    union all select 100000000, 'After Hours'\n    union all select 100000001, 'Qwest'\n    union all select 100000002, 'Customer Portal'\n    union all select 2483, 'Facebook'\n    union all select 3986, 'Twitter')\n    t (SourceId, Source) \"\"\");\n\ndata.registerTempTable(\"tmpIncidentSource\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n  select * from (\n            select 100000003, 'Bell WFM'\n  union all select 100000005, 'CAB Manager'\n  union all select 100000000, 'Comet Tracker'\n  union all select 100000002, 'Encore'\n  union all select 100000007, 'Geotab'\n  union all select 100000010, 'Maxis mWorkforce'\n  union all select 100000008, 'Mobile Warrior ELD'\n  union all select 100000006, 'Mobilise IT'\n  union all select 100000009, 'VisTracks ELD'\n  union all select 100000004, 'Vodafone MWE'\n  union all select 100000001, 'Workforce Manager (AT&T)')\n  t (ProductId, Product) \"\"\");\n\ndata.registerTempTable(\"tmpIncidentProduct\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n  select * from (\n            select 1, 'Question'\n  union all select 2, 'Problem'\n  union all select 3, 'Request')\n  t (CaseTypeCode, CaseType)\n\"\"\");\n\ndata.registerTempTable(\"tmpIncidentCaseType\");"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n  select * from (\n            select 1, 'High'\n  union all select 2, 'Normal'\n  union all select 3, 'Low')\n  t (PriorityCode, Priority)\n\"\"\");\n\ndata.registerTempTable(\"tmpIncidentPriority\");"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["data = spark.sql(\"\"\"\n  select * from (\n            select 100000002, 'Normal'\n  union all select 100000001, 'High'\n  union all select 100000000, 'Urgent')\n  t (SeverityCode, Severity)\n\"\"\");\n\ndata.registerTempTable(\"tmpIncidentSeverity\");"],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"Common","notebookId":2415651579339510},"nbformat":4,"nbformat_minor":0}